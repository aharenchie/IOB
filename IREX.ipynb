{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import pycrfsuite\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# コーパス読み込み\n",
    "import codecs\n",
    "class CorpusReader(object):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        with codecs.open(path, encoding='utf-8') as f:\n",
    "            sent = []\n",
    "            sents = []\n",
    "            for line in f:\n",
    "                if line == '\\n':\n",
    "                    sents.append(sent)\n",
    "                    sent = []\n",
    "                    continue\n",
    "                morph_info = line.strip().split('\\t')\n",
    "                sent.append(morph_info) # 形態素の保存 \n",
    "        train_num = int(len(sents) * 0.9) # 9割を学習に、1割をテストに\n",
    "        self.__train_sents = sents[:train_num]\n",
    "        self.__test_sents = sents[train_num:]\n",
    "        self.__all_sents = sents\n",
    "        \n",
    "    def iob_sents(self, name):\n",
    "        if name == 'train':\n",
    "            return self.__train_sents\n",
    "        elif name == 'test':\n",
    "            return self.__test_sents\n",
    "        elif name == 'all':\n",
    "            return self.__all_sents\n",
    "        else:\n",
    "            return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 文字種取得\n",
    "def is_hiragana(ch):\n",
    "    return 0x3040 <= ord(ch) <= 0x309F \n",
    "    # ひらがな：True or False\n",
    "\n",
    "def is_katakana(ch):\n",
    "    return 0x30A0 <= ord(ch) <= 0x30FF\n",
    "    # カタカタ：True or False\n",
    "\n",
    "def get_character_type(ch): # 文字種を取得する\n",
    "    if ch.isspace(): # 空白の場合\n",
    "        return 'ZSPACE'\n",
    "    elif ch.isdigit(): # 数字の場合\n",
    "        return 'ZDIGIT'\n",
    "    elif ch.islower(): # 小文字の場合\n",
    "        return 'ZLLET'\n",
    "    elif ch.isupper(): # 大文字の場合\n",
    "        return 'ZULET'\n",
    "    elif is_hiragana(ch): # ひらがなの場合\n",
    "        return 'HIRAG'\n",
    "    elif is_katakana(ch): # カタカナの場合\n",
    "        return 'KATAK'\n",
    "    else: # それ以外\n",
    "        return 'OTHER'\n",
    "\n",
    "def get_character_types(string): # 文字列の文字種を変換する\n",
    "    character_types = map(get_character_type, string)\n",
    "    character_types_str = '-'.join(sorted(set(character_types)))\n",
    "\n",
    "    return character_types_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 品詞細分類の取得\n",
    "def extract_pos_with_subtype(morph):\n",
    "    idx = morph.index('*')\n",
    "    return '-'.join(morph[1:idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 単語を特徴量に変換する\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    chtype = get_character_types(sent[i][0]) # 文字種取得\n",
    "    postag = extract_pos_with_subtype(sent[i]) # 品詞分類取得\n",
    "    \n",
    "    # 該当単語の前後2文字の単語の特徴を用意\n",
    "    features = [ \n",
    "        'bias',\n",
    "        'word=' + word,\n",
    "        'type=' + chtype,\n",
    "        'postag=' + postag,\n",
    "    ]\n",
    "    \n",
    "    if i >= 2: # 現在の単語の前に、2単語以上あるとき\n",
    "        word2 = sent[i-2][0]\n",
    "        chtype2 = get_character_types(sent[i-2][0])\n",
    "        postag2 = extract_pos_with_subtype(sent[i-2])\n",
    "        iobtag2 = sent[i-2][-1]\n",
    "        features.extend([\n",
    "            '-2:word=' + word2,\n",
    "            '-2:type=' + chtype2,\n",
    "            '-2:postag=' + postag2,\n",
    "        ])\n",
    "    else: # それ以外は、BOS\n",
    "        features.append('BOS')\n",
    "\n",
    "    if i >= 1: # 現在の単語の前に、1単語以上あるとき\n",
    "        word1 = sent[i-1][0]\n",
    "        chtype1 = get_character_types(sent[i-1][0])\n",
    "        postag1 = extract_pos_with_subtype(sent[i-1])\n",
    "        iobtag1 = sent[i-1][-1]\n",
    "        features.extend([\n",
    "            '-1:word=' + word1,\n",
    "            '-1:type=' + chtype1,\n",
    "            '-1:postag=' + postag1,\n",
    "        ])\n",
    "    else: # それ以外は、BOS\n",
    "        features.append('BOS')\n",
    "\n",
    "    if i < len(sent)-1: # 現在の単語の後ろに、1単語以上あるとき\n",
    "        word1 = sent[i+1][0]\n",
    "        chtype1 = get_character_types(sent[i+1][0])\n",
    "        postag1 = extract_pos_with_subtype(sent[i+1])\n",
    "        features.extend([\n",
    "            '+1:word=' + word1,\n",
    "            '+1:type=' + chtype1,\n",
    "            '+1:postag=' + postag1,\n",
    "        ])\n",
    "    else: # それ以外は、EOS\n",
    "        features.append('EOS')\n",
    "\n",
    "    if i < len(sent)-2: # 現在の単語の後ろに、2単語以上あるとき\n",
    "        word2 = sent[i+2][0]\n",
    "        chtype2 = get_character_types(sent[i+2][0])\n",
    "        postag2 = extract_pos_with_subtype(sent[i+2])\n",
    "        features.extend([\n",
    "            '+2:word=' + word2,\n",
    "            '+2:type=' + chtype2,\n",
    "            '+2:postag=' + postag2,\n",
    "        ])\n",
    "    else: # それ以外は、EOS\n",
    "        features.append('EOS')\n",
    "\n",
    "    return features    \n",
    "\n",
    "def sent2features(sent): # 情報系列から特徴を取得\n",
    "    # 単語ごとに特徴変換していく\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent): # 情報系列からラベル[B、I、O]を取得\n",
    "    return [morph[-1] for morph in sent]\n",
    "\n",
    "def sent2tokens(sent): # 情報系列から単語原文を取得\n",
    "    return [morph[0] for morph in sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ラベル評価\n",
    "def bio_classification_report(y_true, y_pred):\n",
    "    lb = LabelBinarizer()\n",
    "    # 正解ラベルの二値化の保存 (1文字ずつのラベルを二値化に変換)\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    # 予測結果の二値化の保存 (1文字ずつのラベルを二値化に変換)\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))  \n",
    "\n",
    "    tagset = set(lb.classes_) - {'O'} # O以外のタグセットの保存\n",
    "\n",
    "    # B-NAME、I-NAME、B-THEME..等の順番に並び替える\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    # タグのクラスのid化？\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    # 正解ラベルと予想ラベルを引数として、評価表の作成をする\n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ラベルごとに単語を保存する\n",
    "def label_report(text,label_data):\n",
    "    wh_dic = {\"WHERE\":[], \"WHEN\":[], \"WHO\":[], \"WHAT\":[], \"HOW\":[], \"WHY\":[],\"SERIF\":[],\"O\":[]}\n",
    "    \n",
    "    set_label = \"\"\n",
    "    result_word = \"\"\n",
    "    wc = -1\n",
    "    wc_l = []\n",
    "    \n",
    "    for i,word in enumerate(text):\n",
    "        \n",
    "        ld = label_data[i]\n",
    "        \n",
    "        # B,Iタグをとりあえず、無視する\n",
    "        if \"B-\" in ld:\n",
    "            ld = ld.replace(\"B-\", \"\")\n",
    "        elif \"I-\" in ld:\n",
    "            ld = ld.replace(\"I-\", \"\")\n",
    "            \n",
    "        # 最後の要素のとき\n",
    "        if i == (len(text)-1):\n",
    "            wh_dic[set_label].append([result_word,wc_l])\n",
    "            break\n",
    "        \n",
    "        # ラベルごとに辞書へ保存する。\n",
    "        if set_label != \"\" and set_label != ld: \n",
    "            # 辞書に保存            \n",
    "            wh_dic[set_label].append([result_word,wc_l])\n",
    "            \n",
    "            # 初期化\n",
    "            result_word = \"\"\n",
    "            wc_l = []\n",
    "\n",
    "        # 同一ラベルの情報を結合\n",
    "        for wi in range(len(word)):\n",
    "            wc += 1\n",
    "            wc_l.append(wc)\n",
    "        \n",
    "        # 同一ラベルの情報を結合\n",
    "        set_label = ld\n",
    "        result_word += word   \n",
    "            \n",
    "    return wh_dic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "評価用\n",
    "入力：正解データ辞書、予測データ辞書\n",
    "出力： 5W1Hごとの失敗パターン辞書\n",
    "\n",
    "===\n",
    "完全： 正解データ\n",
    "一部一致： 正解データ/予測データ\n",
    "該当なし： 正解データ\n",
    "誤予測： 予測データ\n",
    "\n",
    "\"\"\"\n",
    "def eval_report(true_wh_dic,pred_wh_dic):\n",
    "    report_data = {}\n",
    "    \n",
    "    \n",
    "    for k,v in true_wh_dic.items():\n",
    "        report_v = {}\n",
    "        result_type1 = []\n",
    "        result_type2 = []\n",
    "        result_type3 = []\n",
    "        result_type4 = []\n",
    "        \n",
    "        check_pred = [0 for i in pred_wh_dic[k]]\n",
    "        for true_text in v:    \n",
    "            true_id = true_text[1] \n",
    "\n",
    "            if len(pred_wh_dic[k]) == 0: # 予想辞書にデータが存在しないとき\n",
    "                result_type3.append(true_text[0])\n",
    "                \n",
    "            else: # 予想辞書にデータが存在するとき\n",
    "                check_true = 0 # 正解データが予想辞書データとマッチングしたか確認\n",
    "                for num, pred_text in enumerate(pred_wh_dic[k]):\n",
    "                    pred_id = pred_text[1]                \n",
    "                    match_n = list(set(true_id) & set(pred_id))\n",
    "                    if len(match_n) > 0:\n",
    "                        if len(true_id) == len(pred_id):\n",
    "                            check_pred[num] = 1\n",
    "                            check_true = 1\n",
    "                            result_type1.append(true_text[0]) # 完全\n",
    "                        else:\n",
    "                            check_pred[num] = 1\n",
    "                            check_true = 1\n",
    "                            result_type2.append([true_text[0],pred_text[0]]) #一部一致\n",
    "                            \n",
    "                    if num == (len(pred_wh_dic[k])-1) and check_true == 0: # 正解データが、予想データの最後の要素まで一致しなかったとき\n",
    "                        result_type3.append(true_text[0]) # 該当なし\n",
    "                        \n",
    "        miss_pred = [i for i,c in enumerate(check_pred) if c == 0]\n",
    "        if len(miss_pred) > 0:\n",
    "            for i in miss_pred:\n",
    "                result_type4.append(pred_wh_dic[k][i][0]) # 誤予測\n",
    "            \n",
    "        report_v[\"完全\"] = result_type1\n",
    "        report_v[\"一部一致\"] = result_type2\n",
    "        report_v[\"該当なし\"] = result_type3\n",
    "        report_v[\"誤予測\"] = result_type4            \n",
    "        report_data[k] = report_v\n",
    "        \n",
    "    return report_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4パターンの数を数える\n",
    "def cal_report(file_id,result,wh_label):\n",
    "    wh_num = [0 for i in range(5)]\n",
    "    wh_num[0] = file_id\n",
    "    for wh in wh_label:\n",
    "        for k,v in result[wh].items():\n",
    "            if k == \"完全\":\n",
    "                wh_num[1] += len(v)\n",
    "            elif k == \"一部一致\":\n",
    "                wh_num[2] += len(v)\n",
    "            elif k == \"該当なし\":\n",
    "                wh_num[3] += len(v)\n",
    "            elif k == \"誤予測\":\n",
    "                wh_num[4] += len(v)\n",
    "                    \n",
    "    return wh_num  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【宜野湾】磁石の性質を利用して方位を測定する「磁気コンパス」の誤差修正を担う「コンパスアジャスタ」という資格がある。沖縄県内で唯一、その資格を取得しているのが、宜野湾市愛知に住む１級海技士の上原伸浩さん（６６）だ。\n",
      "==== WHERE ====\n",
      "正解：[['宜野湾', [1, 2, 3]], ['沖縄県内', [58, 59, 60, 61]], ['宜野湾市愛知', [80, 81, 82, 83, 84, 85]]]\n",
      "予想：[['宜野湾', [1, 2, 3]], ['沖縄県内', [58, 59, 60, 61]], ['宜野湾市愛知に住む１級海技士', [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93]]]\n",
      "==== WHO ====\n",
      "正解：[['１級海技士の上原伸浩', [89, 90, 91, 92, 93, 94, 95, 96, 97, 98]]]\n",
      "予想：[['上原伸浩', [95, 96, 97, 98]]]\n",
      "==== WHEN ====\n",
      "正解：[]\n",
      "予想：[]\n",
      "==== WHAT ====\n",
      "正解：[['磁石の性質を利用して方位を測定する「磁気コンパス」の誤差修正を担う「コンパスアジャスタ」という資格', [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]]]\n",
      "予想：[['磁石の性質', [5, 6, 7, 8, 9]], ['方位を測定する「磁気コンパス」の誤差修正を担う「コンパスアジャスタ」', [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]]]\n"
     ]
    }
   ],
   "source": [
    "# サンプルソースコード\n",
    "# 1.データ読み込み\n",
    "#c = CorpusReader('corpus.txt') # ファイル指定\n",
    "c = CorpusReader('corpus_5w1hs.txt') # ファイル指定\n",
    "train_sents = c.iob_sents('train') # データの読み込み\n",
    "test_sents = c.iob_sents('test') # データの読み込み\n",
    "\n",
    "X_train = [sent2features(s) for s in train_sents] # 学習データの特徴量\n",
    "y_train = [sent2labels(s) for s in train_sents] # 学習データのラベル\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents] # テストデータの特徴量\n",
    "y_test = [sent2labels(s) for s in test_sents] # テストデータのラベル\n",
    "\n",
    "# 2.学習\n",
    "trainer = pycrfsuite.Trainer(verbose=False) # モデルの定義\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq) # 学習データの追加\n",
    "    \n",
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "}) # パラメータの設定\n",
    "\n",
    "trainer.train('model.crfsuite') # モデル学習\n",
    "\n",
    "# 3.ラベル予測・評価\n",
    "tagger = pycrfsuite.Tagger() #  pycrfsuiteのモデルを用意\n",
    "tagger.open('model.crfsuite') # モデルを開く\n",
    "tagger.info()\n",
    "\n",
    "# 4.テストの用意\n",
    "example_sent = test_sents[0]\n",
    "text = sent2tokens(example_sent)\n",
    "p_data = tagger.tag(sent2features(example_sent))\n",
    "c_data = sent2labels(example_sent)\n",
    "\n",
    "wh_label = [\"WHERE\",\"WHO\",\"WHEN\",\"WHAT\"]\n",
    "true_wh_dic = label_report(text,c_data)\n",
    "pred_wh_dic = label_report(text,p_data)\n",
    "\n",
    "\n",
    "result = eval_report(true_wh_dic,pred_wh_dic)\n",
    "c_r = cal_report(1,result,wh_label)\n",
    "output_result = []\n",
    "output_result.append(c_r)\n",
    "\n",
    "print(''.join(text))\n",
    "for wh_l in wh_label:\n",
    "    print(\"==== {0} ====\".format(wh_l))\n",
    "    print(\"正解：{0}\".format([t for t in true_wh_dic[wh_l]]))\n",
    "    print(\"予想：{0}\".format([t for t in pred_wh_dic[wh_l]]))\n",
    "\n",
    "#print(\"----------------------\")\n",
    "#y_pred = [tagger.tag(xseq) for xseq in X_test] # テストデータにタグ付け\n",
    "#print(bio_classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ファイル書き込み\n",
    "output_file = \"result_5w1hs.csv\"\n",
    "\n",
    "# 評価結果\n",
    "with open(output_file, 'w') as f:\n",
    "    writer = csv.writer(f, lineterminator='\\n') # 改行コード（\\n）を指定しておく\n",
    "    writer.writerow([\"記事id\",\"完全\",\"一部一致\",\"該当なし\",\"誤予測\"])\n",
    "    for o_data in output_result:\n",
    "        writer.writerow(o_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "# ラベル評価 Leave one out\n",
    "\n",
    "import csv\n",
    "f = open('loo_result_5w1hs.csv', 'w')\n",
    "writer = csv.writer(f, lineterminator='\\n')\n",
    "\n",
    "c = CorpusReader('corpus_5w1hs.txt') # ファイル指定\n",
    "all_sents = c.iob_sents('all') # データの読み込み\n",
    "\n",
    "lb = LabelBinarizer() # one-hot値に変換する\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "for train_index, test_index in loo.split(all_sents):\n",
    "\n",
    "    X_train = [sent2features(all_sents[i]) for i in train_index] # 学習データの特徴量\n",
    "    y_train = [sent2labels(all_sents[i]) for i in train_index] # 学習データのラベル\n",
    "\n",
    "    X_test = [sent2features(all_sents[i]) for i in test_index] # テストデータの特徴量\n",
    "    y_test = [sent2labels(all_sents[i]) for i in test_index] # テストデータのラベル\n",
    "\n",
    "    # 2.学習\n",
    "    trainer = pycrfsuite.Trainer(verbose=False) # モデルの定義\n",
    "\n",
    "    for xseq, yseq in zip(X_train, y_train):\n",
    "        trainer.append(xseq, yseq) # 学習データの追加\n",
    "    \n",
    "    trainer.set_params({\n",
    "        'c1': 1.0,   # coefficient for L1 penalty\n",
    "        'c2': 1e-3,  # coefficient for L2 penalty\n",
    "        'max_iterations': 50,  # stop earlier\n",
    "        # include transitions that are possible, but not observed\n",
    "        'feature.possible_transitions': True\n",
    "    }) # パラメータの設定\n",
    "\n",
    "    trainer.train('model.crfsuite') # モデル学習\n",
    "\n",
    "    # 3.ラベル予測・評価\n",
    "    tagger = pycrfsuite.Tagger() #  pycrfsuiteのモデルを用意\n",
    "    tagger.open('model.crfsuite') # モデルを開く\n",
    "    tagger.info()\n",
    "    \n",
    "    \n",
    "    # 4.テストの用意\n",
    "    example_sent = all_sents[test_index[0]]\n",
    "    \n",
    "    #print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent))))\n",
    "    #print(\"Correct:  \", ' '.join(sent2labels(example_sent)))\n",
    "    predicted = tagger.tag(sent2features(example_sent))\n",
    "    correct = sent2labels(example_sent)\n",
    "    count = 0\n",
    "    for i,p_label in enumerate(predicted):\n",
    "        if p_label == correct[i]:\n",
    "            count += 1    \n",
    "    print(test_index[0])\n",
    "    writer.writerow([test_index[0],count/len(predicted),' '.join(predicted),' '.join(correct),' '.join(sent2tokens(example_sent))])\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
