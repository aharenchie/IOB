{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import pycrfsuite\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# コーパス読み込み\n",
    "import codecs\n",
    "class CorpusReader(object):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        with codecs.open(path, encoding='utf-8') as f:\n",
    "            sent = []\n",
    "            sents = []\n",
    "            for line in f:\n",
    "                if line == '\\n':\n",
    "                    sents.append(sent)\n",
    "                    sent = []\n",
    "                    continue\n",
    "                morph_info = line.strip().split('\\t')\n",
    "                sent.append(morph_info) # 形態素の保存 \n",
    "        train_num = int(len(sents) * 0.9) # 9割を学習に、1割をテストに\n",
    "        self.__train_sents = sents[:train_num]\n",
    "        self.__test_sents = sents[train_num:]\n",
    "        self.__all_sents = sents\n",
    "        \n",
    "    def iob_sents(self, name):\n",
    "        if name == 'train':\n",
    "            return self.__train_sents\n",
    "        elif name == 'test':\n",
    "            return self.__test_sents\n",
    "        elif name == 'all':\n",
    "            return self.__all_sents\n",
    "        else:\n",
    "            return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 文字種取得\n",
    "def is_hiragana(ch):\n",
    "    return 0x3040 <= ord(ch) <= 0x309F \n",
    "    # ひらがな：True or False\n",
    "\n",
    "def is_katakana(ch):\n",
    "    return 0x30A0 <= ord(ch) <= 0x30FF\n",
    "    # カタカタ：True or False\n",
    "\n",
    "def get_character_type(ch): # 文字種を取得する\n",
    "    if ch.isspace(): # 空白の場合\n",
    "        return 'ZSPACE'\n",
    "    elif ch.isdigit(): # 数字の場合\n",
    "        return 'ZDIGIT'\n",
    "    elif ch.islower(): # 小文字の場合\n",
    "        return 'ZLLET'\n",
    "    elif ch.isupper(): # 大文字の場合\n",
    "        return 'ZULET'\n",
    "    elif is_hiragana(ch): # ひらがなの場合\n",
    "        return 'HIRAG'\n",
    "    elif is_katakana(ch): # カタカナの場合\n",
    "        return 'KATAK'\n",
    "    else: # それ以外\n",
    "        return 'OTHER'\n",
    "\n",
    "def get_character_types(string): # 文字列の文字種を変換する\n",
    "    character_types = map(get_character_type, string)\n",
    "    character_types_str = '-'.join(sorted(set(character_types)))\n",
    "\n",
    "    return character_types_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 品詞細分類の取得\n",
    "def extract_pos_with_subtype(morph):\n",
    "    idx = morph.index('*')\n",
    "    return '-'.join(morph[1:idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 単語を特徴量に変換する\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    chtype = get_character_types(sent[i][0]) # 文字種取得\n",
    "    postag = extract_pos_with_subtype(sent[i]) # 品詞分類取得\n",
    "    \n",
    "    # 該当単語の前後2文字の単語の特徴を用意\n",
    "    features = [ \n",
    "        'bias',\n",
    "        'word=' + word,\n",
    "        'type=' + chtype,\n",
    "        'postag=' + postag,\n",
    "    ]\n",
    "    \n",
    "    if i >= 2: # 現在の単語の前に、2単語以上あるとき\n",
    "        word2 = sent[i-2][0]\n",
    "        chtype2 = get_character_types(sent[i-2][0])\n",
    "        postag2 = extract_pos_with_subtype(sent[i-2])\n",
    "        iobtag2 = sent[i-2][-1]\n",
    "        features.extend([\n",
    "            '-2:word=' + word2,\n",
    "            '-2:type=' + chtype2,\n",
    "            '-2:postag=' + postag2,\n",
    "        ])\n",
    "    else: # それ以外は、BOS\n",
    "        features.append('BOS')\n",
    "\n",
    "    if i >= 1: # 現在の単語の前に、1単語以上あるとき\n",
    "        word1 = sent[i-1][0]\n",
    "        chtype1 = get_character_types(sent[i-1][0])\n",
    "        postag1 = extract_pos_with_subtype(sent[i-1])\n",
    "        iobtag1 = sent[i-1][-1]\n",
    "        features.extend([\n",
    "            '-1:word=' + word1,\n",
    "            '-1:type=' + chtype1,\n",
    "            '-1:postag=' + postag1,\n",
    "        ])\n",
    "    else: # それ以外は、BOS\n",
    "        features.append('BOS')\n",
    "\n",
    "    if i < len(sent)-1: # 現在の単語の後ろに、1単語以上あるとき\n",
    "        word1 = sent[i+1][0]\n",
    "        chtype1 = get_character_types(sent[i+1][0])\n",
    "        postag1 = extract_pos_with_subtype(sent[i+1])\n",
    "        features.extend([\n",
    "            '+1:word=' + word1,\n",
    "            '+1:type=' + chtype1,\n",
    "            '+1:postag=' + postag1,\n",
    "        ])\n",
    "    else: # それ以外は、EOS\n",
    "        features.append('EOS')\n",
    "\n",
    "    if i < len(sent)-2: # 現在の単語の後ろに、2単語以上あるとき\n",
    "        word2 = sent[i+2][0]\n",
    "        chtype2 = get_character_types(sent[i+2][0])\n",
    "        postag2 = extract_pos_with_subtype(sent[i+2])\n",
    "        features.extend([\n",
    "            '+2:word=' + word2,\n",
    "            '+2:type=' + chtype2,\n",
    "            '+2:postag=' + postag2,\n",
    "        ])\n",
    "    else: # それ以外は、EOS\n",
    "        features.append('EOS')\n",
    "\n",
    "    return features    \n",
    "\n",
    "def sent2features(sent): # 情報系列から特徴を取得\n",
    "    # 単語ごとに特徴変換していく\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent): # 情報系列からラベル[B、I、O]を取得\n",
    "    return [morph[-1] for morph in sent]\n",
    "\n",
    "def sent2tokens(sent): # 情報系列から単語原文を取得\n",
    "    return [morph[0] for morph in sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ラベル評価\n",
    "def bio_classification_report(y_true, y_pred):\n",
    "    lb = LabelBinarizer()\n",
    "    # 正解ラベルの二値化の保存 (1文字ずつのラベルを二値化に変換)\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    # 予測結果の二値化の保存 (1文字ずつのラベルを二値化に変換)\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))  \n",
    "\n",
    "    tagset = set(lb.classes_) - {'O'} # O以外のタグセットの保存\n",
    "\n",
    "    # B-NAME、I-NAME、B-THEME..等の順番に並び替える\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    # タグのクラスのid化？\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    # 正解ラベルと予想ラベルを引数として、評価表の作成をする\n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ラベルごとに単語を保存する\n",
    "def label_report(text,label_data):\n",
    "    wh_dic = {\"WHERE\":[], \"WHEN\":[], \"WHO\":[], \"WHAT\":[], \"HOW\":[], \"WHY\":[],\"SERIF\":[],\"O\":[]}\n",
    "    \n",
    "    set_label = \"\"\n",
    "    result_word = \"\"\n",
    "    wc = -1\n",
    "    wc_l = []\n",
    "    \n",
    "    for i,word in enumerate(text):\n",
    "        \n",
    "        ld = label_data[i]\n",
    "        \n",
    "        # B,Iタグをとりあえず、無視する\n",
    "        if \"B-\" in ld:\n",
    "            ld = ld.replace(\"B-\", \"\")\n",
    "        elif \"I-\" in ld:\n",
    "            ld = ld.replace(\"I-\", \"\")\n",
    "            \n",
    "        # 最後の要素のとき\n",
    "        if i == (len(text)-1):\n",
    "            wh_dic[set_label].append([result_word,wc_l])\n",
    "            break\n",
    "        \n",
    "        # ラベルごとに辞書へ保存する。\n",
    "        if set_label != \"\" and set_label != ld: \n",
    "            # 辞書に保存            \n",
    "            wh_dic[set_label].append([result_word,wc_l])\n",
    "            \n",
    "            # 初期化\n",
    "            result_word = \"\"\n",
    "            wc_l = []\n",
    "\n",
    "        # 同一ラベルの情報を結合\n",
    "        for wi in range(len(word)):\n",
    "            wc += 1\n",
    "            wc_l.append(wc)\n",
    "        \n",
    "        # 同一ラベルの情報を結合\n",
    "        set_label = ld\n",
    "        result_word += word   \n",
    "            \n",
    "    return wh_dic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "評価用\n",
    "入力：正解データ辞書、予測データ辞書\n",
    "出力： 5W1Hごとの失敗パターン辞書\n",
    "\n",
    "===\n",
    "完全： 正解データ\n",
    "一部一致： 正解データ/予測データ\n",
    "該当なし： 正解データ\n",
    "誤予測： 予測データ\n",
    "\n",
    "\"\"\"\n",
    "def eval_report(true_wh_dic,pred_wh_dic):\n",
    "    report_data = {}\n",
    "    \n",
    "    \n",
    "    for k,v in true_wh_dic.items():\n",
    "        report_v = {}\n",
    "        result_type1 = []\n",
    "        result_type2 = []\n",
    "        result_type3 = []\n",
    "        result_type4 = []\n",
    "        \n",
    "        check_pred = [0 for i in pred_wh_dic[k]]\n",
    "        for true_text in v:    \n",
    "            true_id = true_text[1] \n",
    "\n",
    "            if len(pred_wh_dic[k]) == 0: # 予想辞書にデータが存在しないとき\n",
    "                result_type3.append(true_text[0])\n",
    "                \n",
    "            else: # 予想辞書にデータが存在するとき\n",
    "                check_true = 0 # 正解データが予想辞書データとマッチングしたか確認\n",
    "                for num, pred_text in enumerate(pred_wh_dic[k]):\n",
    "                    pred_id = pred_text[1]                \n",
    "                    match_n = list(set(true_id) & set(pred_id))\n",
    "                    if len(match_n) > 0:\n",
    "                        if len(true_id) == len(pred_id):\n",
    "                            check_pred[num] = 1\n",
    "                            check_true = 1\n",
    "                            result_type1.append(true_text[0]) # 完全\n",
    "                        else:\n",
    "                            check_pred[num] = 1\n",
    "                            check_true = 1\n",
    "                            result_type2.append([true_text[0],pred_text[0]]) #一部一致\n",
    "                            \n",
    "                    if num == (len(pred_wh_dic[k])-1) and check_true == 0: # 正解データが、予想データの最後の要素まで一致しなかったとき\n",
    "                        result_type3.append(true_text[0]) # 該当なし\n",
    "                        \n",
    "        miss_pred = [i for i,c in enumerate(check_pred) if c == 0]\n",
    "        if len(miss_pred) > 0:\n",
    "            for i in miss_pred:\n",
    "                result_type4.append(pred_wh_dic[k][i][0]) # 誤予測\n",
    "            \n",
    "        report_v[\"完全\"] = result_type1\n",
    "        report_v[\"一部一致\"] = result_type2\n",
    "        report_v[\"該当なし\"] = result_type3\n",
    "        report_v[\"誤予測\"] = result_type4            \n",
    "        report_data[k] = report_v\n",
    "        \n",
    "    return report_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4パターンの数を数える\n",
    "def cal_report(file_id,result,wh_label):\n",
    "    wh_num = [0 for i in range(5)]\n",
    "    wh_num[0] = file_id\n",
    "    for wh in wh_label:\n",
    "        for k,v in result[wh].items():\n",
    "            if k == \"完全\":\n",
    "                wh_num[1] += len(v)\n",
    "            elif k == \"一部一致\":\n",
    "                wh_num[2] += len(v)\n",
    "            elif k == \"該当なし\":\n",
    "                wh_num[3] += len(v)\n",
    "            elif k == \"誤予測\":\n",
    "                wh_num[4] += len(v)\n",
    "                    \n",
    "    return wh_num  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4タイプの結果の数を集計する\n",
    "入力：ファイルid、4タイプの結果、ラベル\n",
    "出力：1つのファイルにおける、ラベルごとのタイプごとの結果数\n",
    "\"\"\"\n",
    "def cal_report_wh(file_id,result):\n",
    "    report_data= {}\n",
    "    for label,wh_value in result.items():\n",
    "        \n",
    "        if label == \"O\": # ラベルOは除外する\n",
    "            continue\n",
    "            \n",
    "        wh_num = [0 for i in range(5)]\n",
    "        wh_num[0] = file_id\n",
    "        for rtype,value in wh_value.items():\n",
    "            if rtype == \"完全\":\n",
    "                wh_num[1] += len(value)\n",
    "            elif rtype == \"一部一致\":\n",
    "                wh_num[2] += len(value)\n",
    "            elif rtype == \"該当なし\":\n",
    "                wh_num[3] += len(value)\n",
    "            elif rtype == \"誤予測\":\n",
    "                wh_num[4] += len(value)\n",
    "            \n",
    "        report_data[label] = wh_num\n",
    "        \n",
    "    return report_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# テンプレートを作成する\n",
    "def w2template(pred_wh_dic,text_list):\n",
    "    \n",
    "    template_txt = \"\"\n",
    "    label_ids = []\n",
    "    label_name = {}\n",
    "    \n",
    "    # 4W情報の保存\n",
    "    for k,v in pred_wh_dic.items():\n",
    "        if k == \"O\":\n",
    "            continue\n",
    "            \n",
    "        if len(v) != 0:\n",
    "            for v_i in v:\n",
    "                # 要素の最初と最後\n",
    "                sp_s = v_i[1][0]\n",
    "                sp_e = v_i[1][-1] + 1\n",
    "                \n",
    "                # ラベル情報と要素\n",
    "                label_name[sp_e] = k # ラベル情報保存\n",
    "                label_ids.append(sp_s) # 要素の最初\n",
    "                label_ids.append(sp_e) # 要素の最後\n",
    "                      \n",
    "    # テンプレート生成    \n",
    "    data_text = text_list # 1つめのデータを利用\n",
    "\n",
    "    for i,sp in enumerate(sorted(label_ids)):\n",
    "        \n",
    "        if i == 0: # 最初\n",
    "            if sp != 0:\n",
    "                template_txt += data_text[0:sp]\n",
    "                \n",
    "        elif i == (len(label_ids)-1): # 最後\n",
    "            if sp < len(data_text):\n",
    "                template_txt += \"<{0}>\".format(label_name[sp])\n",
    "                template_txt += data_text[sp:len(data_text)]\n",
    "                \n",
    "        elif i%2 == 0:\n",
    "            template_txt += data_text[start_i:sp]\n",
    "        \n",
    "        elif i%2 != 0:\n",
    "            template_txt += \"<{0}>\".format(label_name[sp])\n",
    "            start_i = sp  \n",
    "            \n",
    "    return template_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n"
     ]
    }
   ],
   "source": [
    "# ラベル評価 Leave one out\n",
    "\n",
    "import csv\n",
    "\n",
    "# ファイル書き出し用\n",
    "ex_file = \"r3_extraction_result.csv\" # ルール3\n",
    "f_file = \"r3_failure_result.csv\" # ルール3\n",
    "ex_txt = \"r3_failure_result.txt\" # ルール3\n",
    "\n",
    "ex_txt_file = open(ex_txt, 'w')  #書き込みモードでオープン\n",
    "output_result = []\n",
    "failure_result = []\n",
    "wh_result = {\"WHERE\":[], \"WHEN\":[], \"WHO\":[], \"WHAT\":[], \"HOW\":[], \"WHY\":[],\"SERIF\":[]}\n",
    "\n",
    "# ファイル読み込み用\n",
    "c = CorpusReader('corpus_5w1hs.txt') # ファイル指定\n",
    "all_sents = c.iob_sents('all') # データの読み込み\n",
    "loo = LeaveOneOut() # LeaveOneOut呼び出し\n",
    "\n",
    "for train_index, test_index in loo.split(all_sents):\n",
    "    \n",
    "    # 1. データ読み込み\n",
    "    X_train = [sent2features(all_sents[i]) for i in train_index] # 学習データの特徴量\n",
    "    y_train = [sent2labels(all_sents[i]) for i in train_index] # 学習データのラベル\n",
    "\n",
    "    X_test = [sent2features(all_sents[i]) for i in test_index] # テストデータの特徴量\n",
    "    y_test = [sent2labels(all_sents[i]) for i in test_index] # テストデータのラベル\n",
    "\n",
    "    # 2.学習\n",
    "    trainer = pycrfsuite.Trainer(verbose=False) # モデルの定義\n",
    "\n",
    "    for xseq, yseq in zip(X_train, y_train):\n",
    "        trainer.append(xseq, yseq) # 学習データの追加\n",
    "    \n",
    "    trainer.set_params({\n",
    "        'c1': 1.0,   # coefficient for L1 penalty\n",
    "        'c2': 1e-3,  # coefficient for L2 penalty\n",
    "        'max_iterations': 50,  # stop earlier\n",
    "        # include transitions that are possible, but not observed\n",
    "        'feature.possible_transitions': True\n",
    "    }) # パラメータの設定\n",
    "\n",
    "    trainer.train('model.crfsuite') # モデル学習\n",
    "\n",
    "    # 3.ラベル予測\n",
    "    tagger = pycrfsuite.Tagger() #  pycrfsuiteのモデルを用意\n",
    "    tagger.open('model.crfsuite') # モデルを開く\n",
    "    tagger.info()\n",
    "    \n",
    "    \n",
    "    # 4.ラベル予測結果\n",
    "    # 4-1. データ用意\n",
    "    test_id = test_index[0]\n",
    "    example_sent = all_sents[test_id] \n",
    "    test_id += 1\n",
    "    text_data = sent2tokens(example_sent) # テキスト\n",
    "    text_line = \"\".join(text_data)\n",
    "    \n",
    "    c_data = sent2labels(example_sent) # 正解ラベルデータ\n",
    "    p_data = tagger.tag(sent2features(example_sent)) # 予想ラベルデータ\n",
    "\n",
    "    true_wh_dic = label_report(text_data,c_data) # 5w1hラベルごとに単語を保存する\n",
    "    pred_wh_dic = label_report(text_data,p_data)  # 5w1hラベルごとに単語を保存する   \n",
    "    result = eval_report(true_wh_dic,pred_wh_dic) # 結果を4パターンで出力\n",
    "    wh_label = [\"WHERE\",\"WHO\",\"WHEN\",\"WHAT\"] # ラベル定義\n",
    "    c_r = cal_report(test_id,result,wh_label) # 4パターンの結果をまとめる\n",
    "    output_result.append(c_r)\n",
    "    \n",
    "    # 結果表示\n",
    "    print(test_id)\n",
    "    ex_txt_file.writelines(\"==== {0} ====\\n\".format(test_id))\n",
    "    ex_txt_file.writelines(\"{0}\\n\".format(text_line))\n",
    "    ex_txt_file.writelines(\"\\n\")\n",
    "    ex_txt_file.writelines(\"{0}\\n\".format(pred_wh_dic))\n",
    "    ex_txt_file.writelines(\"{0}\\n\".format(w2template(pred_wh_dic,text_line)))\n",
    "    \n",
    "    ex_txt_file.writelines(\"-----------------------------\\n\")\n",
    "    for wh_l in wh_label:\n",
    "        ex_txt_file.writelines(\"==== {0} ====\\n\".format(wh_l))\n",
    "        ex_txt_file.writelines(\"正解：{0}\\n\".format([t for t in true_wh_dic[wh_l]]))\n",
    "        ex_txt_file.writelines(\"予想：{0}\\n\".format([t for t in pred_wh_dic[wh_l]]))\n",
    "        ex_txt_file.writelines(\"該当なし：{0}\\n\".format(result[wh_l][\"該当なし\"]))\n",
    "        ex_txt_file.writelines(\"誤予測：{0}\\n\".format(result[wh_l][\"誤予測\"]))\n",
    "   \n",
    "    ex_txt_file.writelines(\"-----------------------------\\n\") \n",
    "    ex_txt_file.writelines(\"{0}\\n\".format([\"記事id\",\"完全\",\"一部一致\",\"該当なし\",\"誤予測\"]))\n",
    "    ex_txt_file.writelines(\"{0}\\n\".format(c_r))\n",
    "\n",
    "    # 1記事における5w1hラベルごとの結果\n",
    "    for k,v in cal_report_wh(test_id,result).items():\n",
    "        wh_result[k].append(v) \n",
    "        \n",
    "    # 失敗データのリスト化\n",
    "    for wh_l in wh_label:\n",
    "        for rt in result[wh_l][\"該当なし\"]:\n",
    "            failure_result.append([test_id,wh_l,\"該当なし\",rt])\n",
    "        for rt in result[wh_l][\"誤予測\"]:\n",
    "            failure_result.append([test_id,wh_l,\"誤予測\",rt])            \n",
    "\n",
    "ex_txt_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ファイル書き込み\n",
    "# 5W1Hラベルごとのデータ\n",
    "\n",
    "for k,v in wh_result.items():\n",
    "    wh_file = \"r3_extraction_resul_{}.csv\".format(k) # ルール3\n",
    "    with open(wh_file, 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n') # 改行コード（\\n）を指定しておく\n",
    "        writer.writerow([\"記事id\",\"完全\",\"一部一致\",\"該当なし\",\"誤予測\"])\n",
    "        for o_data in v:\n",
    "            writer.writerow(o_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ファイル書き込み\n",
    "\n",
    "# 評価結果\n",
    "with open(ex_file, 'w') as f:\n",
    "    writer = csv.writer(f, lineterminator='\\n') # 改行コード（\\n）を指定しておく\n",
    "    writer.writerow([\"記事id\",\"完全\",\"一部一致\",\"該当なし\",\"誤予測\"])\n",
    "    for o_data in output_result:\n",
    "        writer.writerow(o_data)\n",
    "\n",
    "# 失敗データ\n",
    "with open(f_file, 'w') as f:\n",
    "    writer = csv.writer(f, lineterminator='\\n') # 改行コード（\\n）を指定しておく\n",
    "    writer.writerow([\"記事id\",\"ラベル\",\"失敗タイプ\",\"単語\",\"原因\"])\n",
    "    for f_data in failure_result:\n",
    "        writer.writerow(f_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# サンプルソースコード\n",
    "# 1.データ読み込み\n",
    "#c = CorpusReader('corpus.txt') # ファイル指定\n",
    "c = CorpusReader('corpus_5w1hs.txt') # ファイル指定\n",
    "train_sents = c.iob_sents('train') # データの読み込み\n",
    "test_sents = c.iob_sents('test') # データの読み込み\n",
    "\n",
    "X_train = [sent2features(s) for s in train_sents] # 学習データの特徴量\n",
    "y_train = [sent2labels(s) for s in train_sents] # 学習データのラベル\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents] # テストデータの特徴量\n",
    "y_test = [sent2labels(s) for s in test_sents] # テストデータのラベル\n",
    "\n",
    "# 2.学習\n",
    "trainer = pycrfsuite.Trainer(verbose=False) # モデルの定義\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq) # 学習データの追加\n",
    "    \n",
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "}) # パラメータの設定\n",
    "\n",
    "trainer.train('model.crfsuite') # モデル学習\n",
    "\n",
    "# 3.ラベル予測・評価\n",
    "tagger = pycrfsuite.Tagger() #  pycrfsuiteのモデルを用意\n",
    "tagger.open('model.crfsuite') # モデルを開く\n",
    "tagger.info()\n",
    "\n",
    "# 4.テストの用意\n",
    "example_sent = test_sents[0]\n",
    "text_data = sent2tokens(example_sent)\n",
    "p_data = tagger.tag(sent2features(example_sent))\n",
    "c_data = sent2labels(example_sent)\n",
    "\n",
    "wh_label = [\"WHERE\",\"WHO\",\"WHEN\",\"WHAT\"]\n",
    "true_wh_dic = label_report(text_data,c_data)\n",
    "pred_wh_dic = label_report(text_data,p_data)\n",
    "\n",
    "result = eval_report(true_wh_dic,pred_wh_dic)\n",
    "c_r = cal_report(file_id,result,wh_label)\n",
    "\n",
    "\n",
    "# 結果表示\n",
    "text_line = \"\".join(text)\n",
    "print(text_line)\n",
    "print(\" \")\n",
    "print(w2template(pred_wh_dic,text_line))    \n",
    "    \n",
    "print(\"-----------------------------\")    \n",
    "for wh_l in wh_label:\n",
    "    print(\"==== {0} ====\".format(wh_l))\n",
    "    print(\"正解：{0}\".format([t for t in true_wh_dic[wh_l]]))\n",
    "    print(\"予想：{0}\".format([t for t in pred_wh_dic[wh_l]]))\n",
    "    print(\"該当なし：{0}\".format(result[wh_l][\"該当なし\"]))\n",
    "    print(\"誤予測：{0}\".format(result[wh_l][\"誤予測\"]))\n",
    "\n",
    "print(\"-----------------------------\") \n",
    "print([\"記事id\",\"完全\",\"一部一致\",\"該当なし\",\"誤予測\"])\n",
    "print(c_r)    \n",
    "\n",
    "#print(\"----------------------\")\n",
    "#y_pred = [tagger.tag(xseq) for xseq in X_test] # テストデータにタグ付け\n",
    "#print(bio_classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
