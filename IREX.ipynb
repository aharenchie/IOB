{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import pycrfsuite\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# コーパス読み込み\n",
    "import codecs\n",
    "class CorpusReader(object):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        with codecs.open(path, encoding='utf-8') as f:\n",
    "            sent = []\n",
    "            sents = []\n",
    "            for line in f:\n",
    "                if line == '\\n':\n",
    "                    sents.append(sent)\n",
    "                    sent = []\n",
    "                    continue\n",
    "                morph_info = line.strip().split('\\t')\n",
    "                sent.append(morph_info) # 形態素の保存 \n",
    "        train_num = int(len(sents) * 0.9) # 9割を学習に、9割をテストに\n",
    "        print(train_num)\n",
    "        self.__train_sents = sents[:train_num]\n",
    "        self.__test_sents = sents[train_num:]\n",
    "        \n",
    "    def iob_sents(self, name):\n",
    "        if name == 'train':\n",
    "            return self.__train_sents\n",
    "        elif name == 'test':\n",
    "            return self.__test_sents\n",
    "        else:\n",
    "            return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 文字種取得\n",
    "def is_hiragana(ch):\n",
    "    return 0x3040 <= ord(ch) <= 0x309F \n",
    "    # ひらがな：True or False\n",
    "\n",
    "def is_katakana(ch):\n",
    "    return 0x30A0 <= ord(ch) <= 0x30FF\n",
    "    # カタカタ：True or False\n",
    "\n",
    "def get_character_type(ch): # 文字種を取得する\n",
    "    if ch.isspace(): # 空白の場合\n",
    "        return 'ZSPACE'\n",
    "    elif ch.isdigit(): # 数字の場合\n",
    "        return 'ZDIGIT'\n",
    "    elif ch.islower(): # 小文字の場合\n",
    "        return 'ZLLET'\n",
    "    elif ch.isupper(): # 大文字の場合\n",
    "        return 'ZULET'\n",
    "    elif is_hiragana(ch): # ひらがなの場合\n",
    "        return 'HIRAG'\n",
    "    elif is_katakana(ch): # カタカナの場合\n",
    "        return 'KATAK'\n",
    "    else: # それ以外\n",
    "        return 'OTHER'\n",
    "\n",
    "def get_character_types(string): # 文字列の文字種を変換する\n",
    "    character_types = map(get_character_type, string)\n",
    "    character_types_str = '-'.join(sorted(set(character_types)))\n",
    "\n",
    "    return character_types_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 品詞細分類の取得\n",
    "def extract_pos_with_subtype(morph):\n",
    "    idx = morph.index('*')\n",
    "    return '-'.join(morph[1:idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 単語を特徴量に変換する\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    chtype = get_character_types(sent[i][0]) # 文字種取得\n",
    "    postag = extract_pos_with_subtype(sent[i]) # 品詞分類取得\n",
    "    \n",
    "    # 該当単語の前後2文字の単語の特徴を用意\n",
    "    features = [ \n",
    "        'bias',\n",
    "        'word=' + word,\n",
    "        'type=' + chtype,\n",
    "        'postag=' + postag,\n",
    "    ]\n",
    "    \n",
    "    if i >= 2: # 現在の単語の前に、2単語以上あるとき\n",
    "        word2 = sent[i-2][0]\n",
    "        chtype2 = get_character_types(sent[i-2][0])\n",
    "        postag2 = extract_pos_with_subtype(sent[i-2])\n",
    "        iobtag2 = sent[i-2][-1]\n",
    "        features.extend([\n",
    "            '-2:word=' + word2,\n",
    "            '-2:type=' + chtype2,\n",
    "            '-2:postag=' + postag2,\n",
    "            '-2:iobtag=' + iobtag2,\n",
    "        ])\n",
    "    else: # それ以外は、BOS\n",
    "        features.append('BOS')\n",
    "\n",
    "    if i >= 1: # 現在の単語の前に、1単語以上あるとき\n",
    "        word1 = sent[i-1][0]\n",
    "        chtype1 = get_character_types(sent[i-1][0])\n",
    "        postag1 = extract_pos_with_subtype(sent[i-1])\n",
    "        iobtag1 = sent[i-1][-1]\n",
    "        features.extend([\n",
    "            '-1:word=' + word1,\n",
    "            '-1:type=' + chtype1,\n",
    "            '-1:postag=' + postag1,\n",
    "            '-1:iobtag=' + iobtag1,\n",
    "        ])\n",
    "    else: # それ以外は、BOS\n",
    "        features.append('BOS')\n",
    "\n",
    "    if i < len(sent)-1: # 現在の単語の後ろに、1単語以上あるとき\n",
    "        word1 = sent[i+1][0]\n",
    "        chtype1 = get_character_types(sent[i+1][0])\n",
    "        postag1 = extract_pos_with_subtype(sent[i+1])\n",
    "        features.extend([\n",
    "            '+1:word=' + word1,\n",
    "            '+1:type=' + chtype1,\n",
    "            '+1:postag=' + postag1,\n",
    "        ])\n",
    "    else: # それ以外は、BOS\n",
    "        features.append('EOS')\n",
    "\n",
    "    if i < len(sent)-2: # 現在の単語の後ろに、2単語以上あるとき\n",
    "        word2 = sent[i+2][0]\n",
    "        chtype2 = get_character_types(sent[i+2][0])\n",
    "        postag2 = extract_pos_with_subtype(sent[i+2])\n",
    "        features.extend([\n",
    "            '+2:word=' + word2,\n",
    "            '+2:type=' + chtype2,\n",
    "            '+2:postag=' + postag2,\n",
    "        ])\n",
    "    else: # それ以外は、BOS\n",
    "        features.append('EOS')\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent): # 情報系列から特徴を取得\n",
    "    # 単語ごとに特徴変換していく\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "def sent2labels(sent): # 情報系列からラベル[B、I、O]を取得\n",
    "    return [morph[-1] for morph in sent]\n",
    "\n",
    "\n",
    "def sent2tokens(sent): # 情報系列から単語原文を取得\n",
    "    return [morph[0] for morph in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# main \n",
    "c = CorpusReader('corpus.txt') # ファイル指定\n",
    "train_sents = c.iob_sents('train') # データの読み込み\n",
    "test_sents = c.iob_sents('test') # データの読み込み\n",
    "\n",
    "X_train = [sent2features(s) for s in train_sents] # 学習データの特徴量\n",
    "y_train = [sent2labels(s) for s in train_sents] # 学習データのラベル\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents] # テストデータの特徴量\n",
    "y_test = [sent2labels(s) for s in test_sents] # テストデータのラベル\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 学習\n",
    "\n",
    "trainer = pycrfsuite.Trainer(verbose=False) # モデルの定義\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq) # 学習データの追加\n",
    "    \n",
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "}) # パラメータの設定\n",
    "\n",
    "trainer.train('model.crfsuite') # モデル学習\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ラベル評価\n",
    "def bio_classification_report(y_true, y_pred):\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "\n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "\n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ラベル予測・評価\n",
    "\n",
    "tagger = pycrfsuite.Tagger() \n",
    "tagger.open('model.crfsuite') # モデルを開く\n",
    "\n",
    "example_sent = test_sents[1] # テストを指定\n",
    "\n",
    "sent = sent2tokens(example_sent) # 情報系列から単語原文を取得\n",
    "predicted = tagger.tag(sent2features(example_sent)) # 予測ラベル\n",
    "correct = sent2labels(example_sent) # 正解ラベル\n",
    "\n",
    "for s,p,c in zip(sent,predicted,correct):\n",
    "    print(s,p,c)\n",
    "\n",
    "y_pred = [tagger.tag(xseq) for xseq in X_test]\n",
    "print(bio_classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
